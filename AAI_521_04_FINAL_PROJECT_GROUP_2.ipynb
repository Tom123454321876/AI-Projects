{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tom123454321876/AI-Projects/blob/main/AAI_521_04_FINAL_PROJECT_GROUP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Waste Classification with Convolutional Neural Networks - AAI 521  "
      ],
      "metadata": {
        "id": "9GZt-OlXnQnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> Team: Alexander Padin, Daniel Sims, Hassan Ali, Thomas Geraci\n",
        "\n",
        "This project builds a computer vision system that automatically classifies common household waste into categories such as cardboard, glass, metal, paper, plastic, and trash. Using the TrashNet dataset and TensorFlow, we develop three models: a baseline convolutional neural network built from scratch and a two transfer-learning model using a pre-trained architecture like MobileNetV2 and EfficientNetB0. The project includes exploratory data analysis, preprocessing, training, evaluation, and comparison of the models using accuracy, confusion matrices, and class-level metrics.\n",
        "\n",
        "### Dataset\n",
        "The TrashNet dataset contains roughly 2,500 RGB images of common waste items collected using consumer-grade cameras or smartphones. Each image belongs to one of six labeled classes (cardboard, glass, metal, paper, plastic, or trash)and typically shows a single object against a simple background. Images vary naturally in lighting, orientation, and appearance, making the dataset realistic but still manageable for training convolutional neural networks. All images are provided in JPEG format at a resolution of 512x384 pixels, and we apply our own resizing, normalization, and augmentation during preprocessing. The dataset is widely used in academic demos and research on intelligent waste-sorting systems.\n",
        "\n",
        "**Link:** [TrashNet Waste Image Dataset (Kaggle)](https://www.kaggle.com/datasets/feyzazkefe/trashnet)"
      ],
      "metadata": {
        "id": "NS3kyPlbVcpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "3IMBtCvaZfc2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbcxDaExNiAa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "23f4a38e-730e-4ee0-b663-af14e41c0692"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret KAGGLE_USERNAME does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1234290435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m  \u001b[0;31m# Colab secrets API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# add username/token as secrets in colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mKAGGLE_USERNAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KAGGLE_USERNAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mKAGGLE_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KAGGLE_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret KAGGLE_USERNAME does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import PIL.Image as Image\n",
        "from collections import Counter\n",
        "import base64\n",
        "from google.colab import output\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Javascript, display\n",
        "from io import BytesIO\n",
        "import ipywidgets as widgets\n",
        "\n",
        "\n",
        "# setting seed\n",
        "GLOBAL_SEED = 42\n",
        "random.seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "tf.random.set_seed(GLOBAL_SEED)\n",
        "tf.keras.utils.set_random_seed(GLOBAL_SEED)\n",
        "\n",
        "# to get API Keys, goto: https://www.kaggle.com/settings > Create New Token\n",
        "from google.colab import userdata  # Colab secrets API\n",
        "# add username/token as secrets in colab\n",
        "KAGGLE_USERNAME = userdata.get('KAGGLE_USERNAME')\n",
        "KAGGLE_KEY = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfAkecnAtu7A"
      },
      "source": [
        "## Load Dataset\n",
        "In this section we connect Colab to Kaggle, download the TrashNet dataset, and unzip it into the proper directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG1zOuoLQqBW"
      },
      "outputs": [],
      "source": [
        "# download TrashNet dataset form Kaggle\n",
        "url = \"https://www.kaggle.com/api/v1/datasets/download/feyzazkefe/trashnet\"\n",
        "\n",
        "# this requires authentication with Kaggle API keys (https://www.kaggle.com/docs/api#authentication)\n",
        "headers = {\n",
        "    \"Authorization\": f\"Kaggle {KAGGLE_USERNAME}:{KAGGLE_KEY}\"\n",
        "}\n",
        "\n",
        "zip_path = \"/content/trashnet.zip\" # compressed download path\n",
        "\n",
        "# download dataset\n",
        "with requests.get(url, headers=headers, stream=True) as r:\n",
        "    r.raise_for_status()\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "print(\"Downloaded to:\", zip_path)\n",
        "\n",
        "extract_dir = \"/content/trashnet\" # extracted folder path\n",
        "Path(extract_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# unzip\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_dir)\n",
        "print(\"Extracted to:\", extract_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRGBquV8tyF3"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "This section performs quick EDA on the TrashNet images to confirm the dataset's structure and quality. We set `DATA_DIR` to the resized dataset, collect class folders, and extract class names to verify the labels. We inspect the folder structure, then show the first few images from each class in a grid to visually confirm the categories. We also compute image counts per class and plot them to check for imbalance. Finally, we scan all `.jpg` files to collect image sizes and confirm that every image is `512x384`, which means a consistent resize strategy is safe."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Labels"
      ],
      "metadata": {
        "id": "BVqWjkz4Znpl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GSdqGtKlMbv"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"/content/trashnet/dataset-resized\")\n",
        "\n",
        "class_dirs = sorted([p for p in DATA_DIR.iterdir() if p.is_dir()])\n",
        "class_names = [p.name for p in class_dirs]\n",
        "print(\"Classes:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjCiSojXqq4S"
      },
      "outputs": [],
      "source": [
        "# get count per class\n",
        "for c in sorted(DATA_DIR.glob(\"*\")): # for each folder (label)\n",
        "    if c.is_dir(): # count .jpg files\n",
        "        print(c.name, len(list(c.glob(\"*.jpg\"))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Image Sizes"
      ],
      "metadata": {
        "id": "5VcLduRJZvUC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arZttKCFrVk3"
      },
      "outputs": [],
      "source": [
        "# check image sizes\n",
        "sizes = [] # empty list to store sizes\n",
        "\n",
        "for c in DATA_DIR.glob(\"*\"): # for each class\n",
        "    for f in c.glob(\"*.jpg\"): # for each .jpg\n",
        "        img = Image.open(f) # open image\n",
        "        sizes.append(img.size) # store its size (width, height)\n",
        "\n",
        "# count each (width, height)\n",
        "Counter(sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Images and Labels"
      ],
      "metadata": {
        "id": "c1I_QoE8Z5N2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOeKLuVRl9vG"
      },
      "outputs": [],
      "source": [
        "N = 3  # show first 3 images per class\n",
        "\n",
        "plt.figure(figsize=(10, 10)) # size of the display\n",
        "\n",
        "for row, class_dir in enumerate(class_dirs): # for each class\n",
        "    image_paths = sorted(class_dir.glob(\"*.jpg\")) # get all .jpg files for this class sorted by name\n",
        "    sample_paths = image_paths[:N] # take the first N images\n",
        "\n",
        "    for col, img_path in enumerate(sample_paths): # for each N image in the class\n",
        "        idx = row * N + col + 1 # subplot index\n",
        "        plt.subplot(len(class_names), N, idx)\n",
        "\n",
        "        img = mpimg.imread(img_path) # read the image\n",
        "        plt.imshow(img) # show the image\n",
        "        plt.axis(\"off\") # no ticks\n",
        "        plt.title(class_dir.name, fontsize=10) # add class name\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdq74jG3t5LP"
      },
      "source": [
        "## Preprossesing\n",
        "This section builds the TensorFlow input pipeline. We load the dataset using `image_dataset_from_directory`, resize images to `224x224`, and use an 80/10/10 split by first creating a train/validation split and then splitting the validation set in half to form validation and test sets. We confirm the split by printing the number of batches in each dataset. Each dataset is wrapped with `.prefetch(AUTOTUNE)` to speed up training. Preprocessing is handled inside the model using a data-augmentation layer for the training set and normalization layers for all splits. Augmentation is applied only during training, while validation and test inputs are only normalized. By the end, we have clean, batched datasets and a preprocessing pipeline embedded in Keras layers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Datasets"
      ],
      "metadata": {
        "id": "QJSA9DV6bAhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ20MihDBJps"
      },
      "outputs": [],
      "source": [
        "# split dataset 80% train, 10% validation, 10% test\n",
        "\n",
        "img_size = (224, 224) # new size\n",
        "batch_size = 32 # batch size\n",
        "\n",
        "# training set\n",
        "train_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels=\"inferred\", # labels come from subfolder names\n",
        "    label_mode=\"int\", # integer labels\n",
        "    validation_split=0.2, # 20% for validation/test\n",
        "    subset=\"training\", # return training subset\n",
        "    seed=GLOBAL_SEED, # fixed seed for reproducibility\n",
        "    image_size=img_size, # resize 512x384 to 224x224\n",
        "    batch_size=batch_size # images per batch\n",
        ")\n",
        "\n",
        "# temp validation set\n",
        "temp_val_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    validation_split=0.2, # same split\n",
        "    subset=\"validation\", # return validation subset\n",
        "    seed=GLOBAL_SEED,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# split temp validation set into (50% validation/50% test)\n",
        "val_size = tf.data.experimental.cardinality(temp_val_ds_raw) // 2\n",
        "val_ds_raw = temp_val_ds_raw.take(val_size) # validation set\n",
        "test_ds_raw = temp_val_ds_raw.skip(val_size) # test set\n",
        "\n",
        "print(\"\\nTrain batches:\", tf.data.experimental.cardinality(train_ds_raw).numpy())\n",
        "print(\"Val batches:\", tf.data.experimental.cardinality(val_ds_raw).numpy())\n",
        "print(\"Test batches:\", tf.data.experimental.cardinality(test_ds_raw).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Normalization and Augmentation Layers"
      ],
      "metadata": {
        "id": "1RlMinn0bFH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqfv0YqeKFgb"
      },
      "outputs": [],
      "source": [
        "# We decided to bake normalization and augmentation into Keras layers to keep training\n",
        "# and deployment consistent and to make the model easier to share and reuse.\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE # for performance for TF\n",
        "\n",
        "# data augmentation for training dataset only to simulate realistic variations in trash images\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"), # flips\n",
        "        layers.RandomRotation(0.1), # small random rotations\n",
        "        layers.RandomZoom(0.1), # small zoom in/out\n",
        "        layers.RandomContrast(0.1), # change contrast slightly\n",
        "    ],\n",
        "    name=\"data_augmentation\"\n",
        ")\n",
        "\n",
        "# Normalization\n",
        "# CNN model preprocessing function [0, 255] -> [0, 1]\n",
        "normalization_layer_cnn = layers.Rescaling(1.0 / 255.0, name=\"rescale\")\n",
        "\n",
        "# MobileNetV2-specific preprocessing function (scales to [-1, 1])\n",
        "normalization_layer_mobilenet = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "# EfficientNetB0-specific preprocessing function (scales to [-1, 1])\n",
        "normalization_layer_efficientnet = tf.keras.applications.efficientnet.preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm5Pdfe_yZ9Z"
      },
      "source": [
        "## Modeling\n",
        "This section builds and trains three models on the TrashNet dataset. First, we create a baseline CNN trained from scratch using `224x224` images, in-model augmentation, normalization, three Conv2D blocks, and a dense classification head. All models use the Adam optimizer with a learning rate of 1e-3, along with EarlyStopping and ReduceLROnPlateau callbacks. Next, we train a MobileNetV2 transfer-learning model by loading ImageNet weights, freezing the base, applying MobileNetV2 preprocessing, and adding a lightweight classification head. Finally, we train an EfficientNetB0 model with its own preprocessing and a frozen base. All three models use the same train/validation/test splits and consistent callback settings to ensure a fair comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-fetching Datasets"
      ],
      "metadata": {
        "id": "13TQSNuDdKFj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86bwh-Bte5Gr"
      },
      "outputs": [],
      "source": [
        "# Prefetch datasets\n",
        "\n",
        "# training\n",
        "train_ds = (\n",
        "    train_ds_raw\n",
        "    .shuffle(1000, seed=GLOBAL_SEED, reshuffle_each_iteration=True) # add shuffle\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "# validation and test\n",
        "val_ds = val_ds_raw.prefetch(AUTOTUNE)\n",
        "test_ds = test_ds_raw.prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A7b_fKCgq9k"
      },
      "source": [
        "### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ0PSKoEyZUh"
      },
      "outputs": [],
      "source": [
        "# simple CNN built from scratch for Trash classification.\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "\n",
        "  inputs = keras.Input(shape=input_shape, name=\"input_image\") # (height, width, channels)\n",
        "\n",
        "  x = data_augmentation(inputs) # data augmentation when training=True\n",
        "  x = normalization_layer_cnn(x) # normalization [0, 255] to [0, 1]\n",
        "\n",
        "  # convolutional feature extractor\n",
        "  # 1st conv block: 32 filters, 3x3 kernel, ReLU activation, same padding\n",
        "  x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x) # 1st layer: 32 neurons\n",
        "  x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "  # 2nd conv block: 64 filters\n",
        "  x = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x) # 2nd layer: 64 neurons\n",
        "  x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "\n",
        "  # 3rd conv block: 128 filters\n",
        "  x = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x) # 3rd layer: 128 neurons\n",
        "  x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "  # classification head\n",
        "  x = layers.Flatten()(x) # flatten 3D feature maps into a 1D vector\n",
        "  x = layers.Dense(256, activation=\"relu\")(x) # fully connected layer with 256 units\n",
        "  x = layers.Dropout(0.5)(x) # 50% dropout\n",
        "\n",
        "  outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x) # output classification layer\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs, name=\"trashnet_cnn_from_scratch\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyv73ko7gKKg"
      },
      "outputs": [],
      "source": [
        "# build and compile\n",
        "input_shape = (img_size[0], img_size[1], 3)  # (224, 224, 3)\n",
        "num_classes = len(train_ds_raw.class_names) # number of classes\n",
        "\n",
        "# build CNN model\n",
        "cnn_model = build_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# compile CNN model\n",
        "cnn_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3), # learning rate 1e-3\n",
        "    loss=\"sparse_categorical_crossentropy\", # labels are integers (0..num_classes-1)\n",
        "    metrics=[\"accuracy\"], # track accuracy during training and validation\n",
        ")\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HscvET6pgUJQ"
      },
      "outputs": [],
      "source": [
        "epochs = 40 # number of epochs\n",
        "\n",
        "# callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # stops training early if it doesn’t improve for 5 epochs\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5) # cuts the learning rate in half if it stops improving\n",
        "\n",
        "# train the CNN model\n",
        "history = cnn_model.fit(\n",
        "    train_ds, # training dataset (augmented + normalized)\n",
        "    validation_data=val_ds,  # validation dataset (only normalized)\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stop, reduce_lr], # callbacks\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU3QZLpUpmNs"
      },
      "outputs": [],
      "source": [
        "# show learning curve for CNN\n",
        "\n",
        "# extract metrics\n",
        "train_acc = history.history[\"accuracy\"] # training accuracy per epoch\n",
        "val_acc = history.history[\"val_accuracy\"] # validation accuracy per epoch\n",
        "train_loss = history.history[\"loss\"] # training loss per epoch\n",
        "val_loss = history.history[\"val_loss\"] # validation loss per epoch\n",
        "\n",
        "epochs_range = range(1, len(train_acc) + 1)    # 1..N epochs\n",
        "\n",
        "# plot accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# accuracy curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_acc, label=\"Train Accuracy\")\n",
        "plt.plot(epochs_range, val_acc, label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# loss curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_loss, label=\"Train Loss\")\n",
        "plt.plot(epochs_range, val_loss, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUcYReo0gubw"
      },
      "source": [
        "### MobileNetV2 (Transfer Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMdGgrpUul2Y"
      },
      "outputs": [],
      "source": [
        "def build_mobilenet_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Transfer learning model with MobileNetV2:\n",
        "    \"\"\"\n",
        "    # pre-trained base model\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\", # pre-trained on imagenet dataset (https://navigu.net/#imagenet#n03958227/n03958227_12842.jpg)\n",
        "    )\n",
        "    base_model.trainable = False  # freeze base\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape, name=\"input_image\")\n",
        "\n",
        "    x = inputs\n",
        "    # data augmentation (only when training=True)\n",
        "    x = data_augmentation(inputs)\n",
        "\n",
        "    # preprocess for MobileNetV2 (scales/normalizes)\n",
        "    x = normalization_layer_mobilenet(x)\n",
        "\n",
        "    # feature extractor\n",
        "    x = base_model(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # classification head\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mobilenetv2_trashnet\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdJ8tGhTyyND"
      },
      "outputs": [],
      "source": [
        "input_shape = (img_size[0], img_size[1], 3)   # (224, 224, 3)\n",
        "num_classes = len(train_ds_raw.class_names)\n",
        "\n",
        "mobilenet_model = build_mobilenet_model(input_shape, num_classes) # build\n",
        "\n",
        "# compile\n",
        "mobilenet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "mobilenet_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj_elIkCBZR_"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "\n",
        "history_mobilenet = mobilenet_model.fit(\n",
        "    train_ds_raw,\n",
        "    validation_data=val_ds_raw,\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stop, reduce_lr], # same callback as baseline CNN model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHSCEPPXBZPK"
      },
      "outputs": [],
      "source": [
        "# show learning curve for MobileNet model\n",
        "\n",
        "# extract metrics\n",
        "train_acc = history_mobilenet.history[\"accuracy\"] # training accuracy per epoch\n",
        "val_acc = history_mobilenet.history[\"val_accuracy\"] # validation accuracy per epoch\n",
        "train_loss = history_mobilenet.history[\"loss\"] # training loss per epoch\n",
        "val_loss = history_mobilenet.history[\"val_loss\"] # validation loss per epoch\n",
        "\n",
        "epochs_range = range(1, len(train_acc) + 1)    # 1..N epochs\n",
        "\n",
        "# plot accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# accuracy curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_acc, label=\"Train Accuracy\")\n",
        "plt.plot(epochs_range, val_acc, label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# loss curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_loss, label=\"Train Loss\")\n",
        "plt.plot(epochs_range, val_loss, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS3HPv6Vgysr"
      },
      "source": [
        "### EfficientNetB0 (Transfer Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jjDxb7uBZJK"
      },
      "outputs": [],
      "source": [
        "def build_efficientnetb0_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    EfficientNetB0 transfer learning model with:\n",
        "    - In-model data augmentation (train only)\n",
        "    - In-model normalization (always active)\n",
        "    - EfficientNetB0 feature extractor\n",
        "    - Custom classification head\n",
        "    \"\"\"\n",
        "\n",
        "    # raw uint8 [0–255] images\n",
        "    inputs = keras.Input(shape=input_shape, name=\"input_image\")\n",
        "\n",
        "    # data augmentation (only when training=True)\n",
        "    x = data_augmentation(inputs)\n",
        "\n",
        "    # preprocess for MobileNetV2 (scales/normalizes)\n",
        "    x = normalization_layer_efficientnet(x)\n",
        "\n",
        "    # EfficientNetB0 backbone\n",
        "    base_model = EfficientNetB0(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_tensor=x,  # pass augmented+normalized tensor\n",
        "        name=\"efficientnetb0\"\n",
        "    )\n",
        "    base_model.trainable = False  # freeze CNN\n",
        "\n",
        "    # classification head\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs, name=\"trashnet_efficientnetb0\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecwt8Vx_Vzxl"
      },
      "outputs": [],
      "source": [
        "# build and compile the model\n",
        "input_shape = (img_size[0], img_size[1], 3)  # (224, 224, 3)\n",
        "num_classes = len(train_ds_raw.class_names)\n",
        "\n",
        "# build\n",
        "effnet_model = build_efficientnetb0_model(input_shape, num_classes)\n",
        "\n",
        "# compile\n",
        "effnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "effnet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMQMq6lmBZGD"
      },
      "outputs": [],
      "source": [
        "epochs = 40  # num of epochs\n",
        "\n",
        "history_effnet = effnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stop, reduce_lr],  # same callback as baseline CNN model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3wWC-iIBY_T"
      },
      "outputs": [],
      "source": [
        "# show learning curve for EfficientNetB0 model\n",
        "\n",
        "# extract metrics\n",
        "train_acc = history_effnet.history[\"accuracy\"] # training accuracy per epoch\n",
        "val_acc = history_effnet.history[\"val_accuracy\"] # validation accuracy per epoch\n",
        "train_loss = history_effnet.history[\"loss\"] # training loss per epoch\n",
        "val_loss = history_effnet.history[\"val_loss\"] # validation loss per epoch\n",
        "\n",
        "epochs_range = range(1, len(train_acc) + 1) # 1..N epochs\n",
        "\n",
        "# plot accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# accuracy curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_acc, label=\"Train Accuracy\")\n",
        "plt.plot(epochs_range, val_acc, label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# loss curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_loss, label=\"Train Loss\")\n",
        "plt.plot(epochs_range, val_loss, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2aBuu3Dy-PX"
      },
      "source": [
        "## Validation and Performance Metrics\n",
        "This section evaluates the trained models and compares their performance. We start by running `model.evaluate(test_ds)` for the CNN, MobileNetV2, and EfficientNetB0 to report test loss and accuracy for each. We then compute confusion matrices by collecting predictions across the test set and visualize them to see where each model struggles, such as confusing similar classes. We also generate classification reports that include precision, recall, and f1-scores for every class. Finally, we summarize which model performs best overall, how transfer learning compares to the from-scratch CNN, and any trade-offs we observed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "9cqjkfFvfwu6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKQm3EFpy2qL"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = cnn_model.evaluate(test_ds, verbose=0)\n",
        "test_loss_mobilenet, test_acc_mobilenet = mobilenet_model.evaluate(test_ds, verbose=0)\n",
        "test_loss_effnet, test_acc_effnet = effnet_model.evaluate(test_ds, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBa9uEHtlTQS"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_names = [\"CNN\", \"MobileNetV2\", \"EfficientNetB0\"]\n",
        "test_accuracies = [test_acc, test_acc_mobilenet, test_acc_effnet]\n",
        "test_losses = [test_loss, test_loss_mobilenet, test_loss_effnet]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# accuracy bar chart\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(x, test_accuracies)\n",
        "plt.xticks(x, model_names, rotation=20)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Test Accuracy by Model\")\n",
        "\n",
        "# loss bar chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(x, test_losses)\n",
        "plt.xticks(x, model_names, rotation=20)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Test Loss by Model\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print\n",
        "for name, acc, loss in zip(model_names, test_accuracies, test_losses):\n",
        "    print(f\"{name}: Test Acc ({acc:.4f}) Test Loss ({loss:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrices"
      ],
      "metadata": {
        "id": "HwDHyeLNgAGE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAhstcICy9x7"
      },
      "outputs": [],
      "source": [
        "# classification matrices\n",
        "def get_cm(model):\n",
        "  \"\"\"Compute confusion matrix for a given model on test_ds.\"\"\"\n",
        "  y_true, y_pred = [], []\n",
        "\n",
        "  for images_batch, labels_batch in test_ds: # for each test dataset batches\n",
        "      probs = model.predict(images_batch, verbose=0) # (batch_size, num_classes)\n",
        "      batch_preds = np.argmax(probs, axis=1) # predicted class indices\n",
        "\n",
        "      # add labels and predictions to the lists\n",
        "      y_true.extend(labels_batch.numpy())\n",
        "      y_pred.extend(batch_preds)\n",
        "\n",
        "  # convert lists to numpy arrays\n",
        "  y_true = np.array(y_true)\n",
        "  y_pred = np.array(y_pred)\n",
        "\n",
        "  # get confusion matrix\n",
        "  return confusion_matrix(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap6L52UfrOby"
      },
      "outputs": [],
      "source": [
        "# compute confusion matrices\n",
        "cm_cnn = get_cm(cnn_model)\n",
        "cm_mobilenet = get_cm(mobilenet_model)\n",
        "cm_effnet = get_cm(effnet_model)\n",
        "\n",
        "# create 1x3 grid of subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
        "\n",
        "# CNN\n",
        "disp_cnn = ConfusionMatrixDisplay(confusion_matrix=cm_cnn, display_labels=class_names)\n",
        "disp_cnn.plot(\n",
        "    ax=axes[0],\n",
        "    cmap=\"Blues\",\n",
        "    values_format=\"d\",\n",
        "    xticks_rotation=45,\n",
        "    colorbar=False\n",
        ")\n",
        "axes[0].set_title(\"CNN From Scratch\")\n",
        "axes[0].set_xlabel(\"Predicted\")\n",
        "axes[0].set_ylabel(\"True\")\n",
        "\n",
        "# MobileNetV2\n",
        "disp_mobilenet = ConfusionMatrixDisplay(confusion_matrix=cm_mobilenet, display_labels=class_names)\n",
        "disp_mobilenet.plot(\n",
        "    ax=axes[1],\n",
        "    cmap=\"Greens\",\n",
        "    values_format=\"d\",\n",
        "    xticks_rotation=45,\n",
        "    colorbar=False\n",
        ")\n",
        "axes[1].set_title(\"MobileNetV2 Transfer Learning\")\n",
        "axes[1].set_xlabel(\"Predicted\")\n",
        "axes[1].set_ylabel(\"True\")\n",
        "\n",
        "# EfficientNetB0\n",
        "disp_effnet = ConfusionMatrixDisplay(confusion_matrix=cm_effnet, display_labels=class_names)\n",
        "disp_effnet.plot(\n",
        "    ax=axes[2],\n",
        "    cmap=\"Purples\",\n",
        "    values_format=\"d\",\n",
        "    xticks_rotation=45,\n",
        "    colorbar=False\n",
        ")\n",
        "axes[2].set_title(\"EfficientNetB0 Transfer Learning\")\n",
        "axes[2].set_xlabel(\"Predicted\")\n",
        "axes[2].set_ylabel(\"True\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling Results and Findings\n",
        "\n",
        "The overall best performer is EfficientNetB0, which achieves the highest test accuracy and the lowest test loss. Its confusion matrix shows fewer misclassifications across most classes, especially for glass, paper, and metal. MobileNetV2 also performs well with a moderate test loss, and its errors are noticeably lower than the CNN's, confirming that transfer learning provides a meaningful boost. The CNN trained from scratch performs the worst and shows more cross-class confusion, which is expected given the limited size of TrashNet and the fact that it must learn features without pretrained knowledge.\n",
        "\n",
        "The trade-offs are straightforward: the baseline CNN is simpler and trains quickly but underfits; MobileNetV2 is lightweight and accurate; EfficientNetB0 offers the strongest performance but is slightly heavier and takes longer to train. Overall, transfer learning clearly outperforms the from-scratch model on both accuracy and stability, with EfficientNetB0 providing the best balance of feature quality and generalization."
      ],
      "metadata": {
        "id": "NGnnEtQcgaAw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fafnBaU6zC38"
      },
      "source": [
        "## Fine Tunning Selected Model\n",
        "To further improve the best-performing model, EfficientNetB0, we performed a targeted fine-tuning stage. After training the model with its pretrained ImageNet weights frozen, we selectively unfroze the deeper EfficientNet blocks while keeping earlier layers fixed. This let the network adapt high-level feature representations specifically to TrashNet without disrupting the low-level filters that were already well-generalized. We also lowered the learning rate during fine-tuning to avoid damaging pretrained weights, kept the Adam optimizer at `1e-3` for stability, and continued using EarlyStopping and ReduceLROnPlateau to control overfitting. This approach allowed EfficientNetB0 to learn more dataset-specific patterns—especially for visually similar classes—resulting in better generalization and the highest test accuracy among all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl6CL_2bKDPN"
      },
      "outputs": [],
      "source": [
        "# Fine tuning selected Model\n",
        "\n",
        "for layer in effnet_model.layers: # freeze all layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# unfreeze the last N non-BatchNorm layers (e.g., last 50)\n",
        "N = 50\n",
        "for layer in effnet_model.layers[-N:]:\n",
        "    if not isinstance(layer, layers.BatchNormalization):\n",
        "        layer.trainable = True\n",
        "\n",
        "# re-compile with a smaller learning rate\n",
        "effnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# continue training\n",
        "fine_tune_epochs = 20  # or 20, etc.\n",
        "initial_epoch = len(history_effnet.history[\"loss\"]) # starting from the last training\n",
        "\n",
        "# train again, now with some EfficientNet layers unfrozen (fine-tuning phase)\n",
        "history_effnet_ft = effnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=initial_epoch + fine_tune_epochs, # total epochs = previous + new\n",
        "    initial_epoch=initial_epoch, # start counting from previous last epoch\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-WTrpAnO1Bl"
      },
      "outputs": [],
      "source": [
        "# show learning curve for fine-tuned EfficientNetB0 model\n",
        "\n",
        "# extract metrics\n",
        "train_acc = history_effnet_ft.history[\"accuracy\"] # training accuracy per epoch\n",
        "val_acc = history_effnet_ft.history[\"val_accuracy\"] # validation accuracy per epoch\n",
        "train_loss = history_effnet_ft.history[\"loss\"] # training loss per epoch\n",
        "val_loss = history_effnet_ft.history[\"val_loss\"] # validation loss per epoch\n",
        "\n",
        "epochs_range = range(1, len(train_acc) + 1) # 1..N epochs\n",
        "\n",
        "# plot accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# accuracy curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_acc, label=\"Train Accuracy\")\n",
        "plt.plot(epochs_range, val_acc, label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# loss curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_loss, label=\"Train Loss\")\n",
        "plt.plot(epochs_range, val_loss, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ83DPOVO0pX"
      },
      "outputs": [],
      "source": [
        "test_loss_effnet, test_acc_effnet = effnet_model.evaluate(test_ds)\n",
        "print(f\"Fine-tuned EfficientNetB0 - Test accuracy: {test_acc_effnet:.4f}\")\n",
        "print(f\"Fine-tuned EfficientNetB0 - Test loss: {test_loss_effnet:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classification report for fine-tuned EfficientNetB0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    preds = effnet_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "id": "AWdzIMYHmOHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fau-hUuO06k"
      },
      "outputs": [],
      "source": [
        "# compute confusion matrix for fine-tuned EfficientNetB0\n",
        "cm_effnet = get_cm(effnet_model)\n",
        "\n",
        "# create a single subplot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "# EfficientNetB0\n",
        "disp_effnet = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm_effnet,\n",
        "    display_labels=class_names\n",
        ")\n",
        "disp_effnet.plot(\n",
        "    ax=ax,\n",
        "    cmap=\"Purples\",\n",
        "    values_format=\"d\",\n",
        "    xticks_rotation=45,\n",
        "    colorbar=False\n",
        ")\n",
        "\n",
        "ax.set_title(\"Fine-tuned EfficientNetB0 Transfer Learning\")\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning EfficientNetB0 improved performance beyond the initial transfer-learning stage. The confusion matrix shows cleaner predictions across all classes, with fewer misclassifications. The training curves indicate steady improvement in training accuracy and decreasing training loss, while validation accuracy remains stable without signs of severe overfitting."
      ],
      "metadata": {
        "id": "ZCexsGpmiGw6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fyQ7IaAzJRd"
      },
      "source": [
        "## Deploy\n",
        "In the Deploy section, we turned our fine-tuned EfficientNetB0 model into a live webcam classifier running directly in Colab. Using the structure provided in Assignments 4 and 5, we adapted and expanded the code to fit our waste-classification task. The `classify_effnet` function processes a single frame by converting it from BGR to RGB, resizing it, running it through the model, applying softmax, and returning the top predicted label with its confidence. The detect_objects function overlays that prediction on each incoming frame using OpenCV. For real-time video, we reused and modified the JavaScript webcam pipeline from the previous assignments. `capture_video()` streams frames from the browser to Python, while the Python callback decodes each frame, runs EfficientNetB0 classification, draws the label, re-encodes the image, and updates a `widgets.Image` display. With these adaptations from earlier coursework, we created a working demo that performs real-time TrashNet classification on live webcam input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLgUdUv8ac1c"
      },
      "outputs": [],
      "source": [
        "# classifier for a single frame\n",
        "def classify_effnet(frame_bgr):\n",
        "  \"\"\"\n",
        "  frame_bgr: OpenCV BGR image (H, W, 3)\n",
        "  returns (label, confidence)\n",
        "  \"\"\"\n",
        "\n",
        "  # make a copy to avoid modifying original\n",
        "  img = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB) # to RGB\n",
        "  img = cv2.resize(img, img_size)\n",
        "\n",
        "  x = np.expand_dims(img, axis=0).astype(\"float32\")\n",
        "\n",
        "  # use our fine-tuned model\n",
        "  preds = effnet_model.predict(x, verbose=0)[0] # logits or probs\n",
        "  probs = tf.nn.softmax(preds).numpy() # to probabilities\n",
        "\n",
        "  idx = int(np.argmax(probs))\n",
        "  label = class_names[idx]\n",
        "  conf = float(probs[idx])\n",
        "\n",
        "  return label, conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10Y3GMAfe8sC"
      },
      "outputs": [],
      "source": [
        "# detect an object to classify\n",
        "def detect_objects(frame_bgr):\n",
        "  \"\"\"\n",
        "  Takes BGR frame, runs EfficientNet classification,\n",
        "  overlays label + confidence, and returns annotated BGR frame.\n",
        "  \"\"\"\n",
        "\n",
        "  # get label and confidence\n",
        "  label, conf = classify_effnet(frame_bgr)\n",
        "  text = f\"{label}: {conf*100:.1f}%\"\n",
        "\n",
        "  # add to frame\n",
        "  cv2.putText(\n",
        "    frame_bgr,\n",
        "    text,\n",
        "    (10, 30),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "    1.0,\n",
        "    (0, 0, 0),\n",
        "    2,\n",
        "    cv2.LINE_AA\n",
        "  )\n",
        "\n",
        "  return frame_bgr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3-N7ErKfYw3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# JS to capture video - used Assignment 5 implementation\n",
        "def capture_video():\n",
        "    display(Javascript('''\n",
        "        async function captureVideo() {\n",
        "            const video = document.createElement('video');\n",
        "            const canvas = document.createElement('canvas');\n",
        "            const context = canvas.getContext('2d');\n",
        "            document.body.appendChild(video);\n",
        "            video.style.display = 'none';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "            video.srcObject = stream;\n",
        "            await new Promise((resolve) => video.onloadedmetadata = resolve);\n",
        "            video.play();\n",
        "\n",
        "            const [track] = stream.getVideoTracks();\n",
        "            const settings = track.getSettings();\n",
        "            canvas.width = settings.width;\n",
        "            canvas.height = settings.height;\n",
        "\n",
        "            async function stopVideo() {\n",
        "                stream.getTracks().forEach(track => track.stop());\n",
        "                video.remove();\n",
        "                canvas.remove();\n",
        "            }\n",
        "\n",
        "            window.stopVideo = stopVideo;\n",
        "\n",
        "            while (true) {\n",
        "                context.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "                const imageData = canvas.toDataURL('image/jpeg', 0.8);\n",
        "                const response = await google.colab.kernel.invokeFunction('notebook.getFrame', [imageData], {});\n",
        "                if (response.data.result === 'quit') break;\n",
        "                await new Promise((resolve) => setTimeout(resolve, 100));\n",
        "            }\n",
        "\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            video.remove();\n",
        "            canvas.remove();\n",
        "        }\n",
        "        captureVideo();\n",
        "    '''))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtJnMKgufdlR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Python callback: get_frame\n",
        "def get_frame(image_data):\n",
        "  try:\n",
        "    image_data = image_data.split(\",\")[1]\n",
        "    image_bytes = base64.b64decode(image_data)\n",
        "    image = Image.open(BytesIO(image_bytes)) # PIL RGB\n",
        "    frame = np.array(image) # RGB\n",
        "\n",
        "    # convert to BGR for OpenCV\n",
        "    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # run EfficientNet classification + overlay\n",
        "    frame_annotated = detect_objects(frame_bgr)\n",
        "\n",
        "    # encode back to JPEG for the widget (expects BGR)\n",
        "    _, img_encoded = cv2.imencode('.jpg', frame_annotated)\n",
        "    image_widget.value = img_encoded.tobytes()\n",
        "\n",
        "    return {'result': 'continue'}\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing frame: {e}\")\n",
        "    return {'result': 'error'}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xgWW8hhfiLA"
      },
      "outputs": [],
      "source": [
        "\n",
        "output.register_callback('notebook.getFrame', get_frame)\n",
        "\n",
        "image_widget = widgets.Image(format='jpeg')\n",
        "display(image_widget)\n",
        "\n",
        "def stop_video(change):\n",
        "  eval_js('window.stopVideo()')\n",
        "  return {'result': 'quit'}\n",
        "\n",
        "stop_button = widgets.Button(description=\"Stop Video\")\n",
        "stop_button.on_click(stop_video)\n",
        "display(stop_button)\n",
        "\n",
        "capture_video() # deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "This project successfully implemented an end-to-end waste-classification system using deep learning and computer vision. Through systematic experimentation, we built three models (a baseline CNN, MobileNetV2, and EfficientNetB0) using consistent preprocessing and training settings to ensure a fair comparison. Transfer learning proved to be a major advantage: both pretrained models outperformed the CNN from scratch, with EfficientNetB0 achieving the strongest overall performance.\n",
        "\n",
        "Fine-tuning EfficientNetB0 pushed the model even further, improving accuracy and reducing test loss by allowing deeper layers to adapt specifically to TrashNet's image distribution. The confusion matrices and classification reports confirmed better class separation and fewer misclassifications after fine-tuning.\n",
        "\n",
        "Finally, we deployed the model in a real-time webcam demo. This created a functional prototype capable of detecting and labeling waste categories live, demonstrating how a research model can translate into an interactive application."
      ],
      "metadata": {
        "id": "t6KcmdQumncY"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}